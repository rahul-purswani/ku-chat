# KU-Chat: Finetuning Llama on custom data for QA tasks
## Overview
KU-Chat is a KU-focused question answering model built by fine-tuning Meta’s LLaMA-3.2-1B-Instruct using [LoRA (Low-Rank Adaptation)] and [PEFT]. It uses custom QAC pairs from KU’s website and supports both contextual and non-contextual inference.

## Inference Demo
https://github.com/user-attachments/assets/a30f6f76-4990-44a4-be7f-988a3f547d95

## BERT Scores
![image](https://github.com/user-attachments/assets/4107affa-e35d-4520-9472-8e45067c1631)

## ROUGE Scores
![image](https://github.com/user-attachments/assets/601d3e17-68c7-4af1-93c8-5e69153abc14)
